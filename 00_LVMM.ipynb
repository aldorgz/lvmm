{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![LVMM logo](https://lvmm.mx/wp-content/uploads/2019/05/LVMM_logo_500-90x90.png)\n",
        "\n",
        "# Manuales de uso del clúster LVMM"
      ],
      "metadata": {
        "id": "JPm6Ci2B0JLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ssh\"></a>\n",
        "## 1. ¿Cómo acceder al cluster LVMM?\n",
        "\n",
        "La coneción al clúster computacional del Departamento de Modelación de Nanomateriales es por medio del protocolo [OpenSSH](https://www.openssh.com/) (Open Secure Shell, por sus siglas en inglés), dentro de la red del CNyN ésto se realiza por medio del siguiente comando:\n",
        "\n",
        "```shell\n",
        "$ ssh usuario@192.168.100.237\n",
        "```\n",
        "\n",
        "donde `usuario` se refiere al nombre del usuario al que se quiere acceder en el clúster, con éste comando nos preguntará nuestra clave:\n",
        "\n",
        "```shell\n",
        "By accessing this system, you consent to the following conditions:\n",
        "- This system is for authorized use only.\n",
        "- Any or all uses of this system and all files on this system may be monitored.\n",
        "- Communications using, or data stored on, this system are not private.\n",
        "\n",
        "usuario@192.168.100.237's password:\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "12NLSVz6FB9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"module\"></a>\n",
        "# 2. ¿Cómo utilizar los programas instalados en el clúster LVMM?\n",
        "\n",
        "<a name=\"moduleav\"></a>\n",
        "Para poder utilizar los programas ya instalabos en el cúster LVMM es necesario cargar el módulo adecuado, ([Environment Modules](https://modules.sourceforge.net/)), la lista de aplicaciones, librerías y utilidades, se pueden consultar con el siguiente comando:\n",
        "\n",
        "```shell\n",
        "$ module available\n",
        "```\n",
        "\n",
        "o de forma adbreviada:\n",
        "\n",
        "```shell\n",
        "$ module av\n",
        "```\n",
        "\n",
        "```----------------------------------- /share/modules/Applications ------------------------------------\n",
        "abinit/10.0.7.1-intel    gromacs/2024.1-gnu-mkl     vasp/6.4.1-intel           \n",
        "amber/20-Update12        irrep/1.1                  vasp/6.4.1-intel-vtst         \n",
        "autodock/4.2.6-intel     lammps/2Aug2023-intel      vasp/6.4.1-MD-intel           \n",
        "boltztrap/20.7.1         namd/2.14-intel            vasp/6.4.1-nvhpc-gpu            \n",
        "cp2k/2024.1-gnu-mkl      py4vasp/0.5.1              wannier90/2.1.0-intel\n",
        "critic2/1.1.git-intel    qe/7.1-intel               wannier90/3.1.0-intel         \n",
        "crystal17/1.0.2-intel    siesta/honpas-4.1.5-intel  wannier90/3.1.0-serial-intel  \n",
        "dftb+/24.1-intel         tinker/8.10.2-intel        wien2k/23.2-intel             \n",
        "gromacs/2024.1-gnu-cuda  \n",
        "\n",
        "Key:\n",
        "default-version  modulepath  \n",
        "```\n",
        "\n",
        "Si queremos utilizar, por ejemplo, el programa VASP para GPU, debemos primero cargar el módulo `vasp/6.4.1-nvhpc-gpu` con el siguiente comando:\n",
        "\n",
        "```{coda-shell}{shell}\n",
        "$ module load vasp/6.4.1-nvhpc-gpu\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "Loading vasp version 6.4.1\n",
        "Loading mkl version 2023.0.0\n",
        "Loading tbb version 2021.8.0\n",
        "Loading compiler-rt version 2023.0.0\n",
        "Loading fftw version 3.3.10\n",
        "Loading hdf5 version 1.14.3\n",
        "\n",
        "Loading vasp/6.4.1-nvhpc-gpu\n",
        "  Loading requirement: nvhpc/24.5 tbb/latest compiler-rt/latest mkl/latest fftw/3.3.10-nvhpc\n",
        "    hdf5/1.14.3-nvhpc\n",
        "```\n",
        "\n",
        "Los mensajes de salida indican que también se han cargado las dependencias requeridas para que funcione correctamente nuestra aplicación, una vez realizado ésto podémos correr `vasp_std`, `vasp_ncl` o `vasp_gam`.\n",
        "\n",
        "```\n",
        "$ mpirun -n 4 vasp_ncl\n",
        " running    4 mpi-ranks, with    2 threads/rank, on    1 nodes\n",
        " distrk:  each k-point on    4 cores,    1 groups\n",
        " distr:  one band on    1 cores,    4 groups\n",
        " vasp.6.4.1 05Apr23 (build Jul  1 2024 01:14:50) complex                         \n",
        " ```\n",
        "\n",
        "con éste comando se ejecuta el programa `vasp_ncl` en el **nodo maestro**, lo que **NO es recomendado**)."
      ],
      "metadata": {
        "id": "ApIATjmlGWGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"slurm\"></a>\n",
        "# 3. ¿Cómo correr una aplicación en el LVMM?\n",
        "\n",
        "El clúster HPC LVMM cuenta con el sistema de administración de recursos [SLURM](https://slurm.schedmd.com) (Simple Linux Utility for Resource Management, por sus siglas en inglés), con el comando `sinfo` podemos obtener información sobre el cluster,\n",
        "\n",
        "<a name=\"sinfo\"></a>\n",
        "## 3.1 sinfo\n",
        "```shell\n",
        "$ sinfo\n",
        "```\n",
        "```shell\n",
        "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
        "gpu          up   infinite      2   idle compute-0-[0,5]\n",
        "mem          up   infinite      1    mix compute-0-1\n",
        "cpu*         up   infinite      3    mix compute-0-[1-2,4]\n",
        "cpu*         up   infinite      1  alloc compute-0-3\n",
        "```\n",
        "éste comando nos muestra las **particiones** (colar) de ejecución de trabajos, podemos ver que ésta instalación de `SLURM` cuenta con 3 particiones distintas: `gpu`, `mem` y `cpu`, también podemos ver el éstado de los nodos de éstas particiones; La partición `gpu` se encuentra **desocupada** y que cuenta de los nodos de cómputo: `compute-0-0` y `compute-0-5`; también podemos ver que de la partición `cpu`, el nodo `compute-0-3` se encuentra **ocupado** y los nodos `compute-0-1`, `compute-0-2` y el nodo `compute-0-4` se encuentran en un éstado de ocupación **mixto**.\n",
        "\n",
        "Podemos obtener la misma información pero ahora centrada en los nodos en lugar de las particiones con la opción `--Node` o en la forma abreviada `--N`,\n",
        "\n",
        "```shell\n",
        "$ sinfo --Node\n",
        "NODELIST     NODES PARTITION STATE\n",
        "compute-0-0      1       gpu idle  \n",
        "compute-0-1      1      cpu* mix   \n",
        "compute-0-1      1       mem mix   \n",
        "compute-0-2      1      cpu* mix   \n",
        "compute-0-3      1      cpu* alloc\n",
        "compute-0-4      1      cpu* mix   \n",
        "compute-0-5      1       gpu idle  \n",
        "```\n",
        "\n",
        "<a name=\"srun\"></a>\n",
        "## 3.2 srun\n",
        "\n",
        "Para correr `vasp` en el cluster se puede ejecutar con la instrucción `srun` de la siguiente manera:\n",
        "\n",
        "```shell\n",
        "$ srun --partition mem --ntasks 16 vasp_ncl\n",
        "```\n",
        "```shell\n",
        " running   16 mpi-ranks, on    1 nodes\n",
        " distrk:  each k-point on   16 cores,    1 groups\n",
        " distr:  one band on    4 cores,    4 groups\n",
        " vasp.6.4.1 05Apr23 (build Feb 23 2024 02:44:29) complex                        \n",
        "```\n",
        "\n",
        "con ésto corremos el programa `vasp_ncl` en la partición `mem` con 15 tarea, el inconveniente de ésta forma de correr el programa es que tenemos que esperar hasta que estén disponibles los recursos requeridos y durante su ejecución.\n",
        "\n",
        "<a name=\"sbatch\"></a>\n",
        "## 3.3 sbatch\n",
        "\n",
        "La forma idonea para mandar un trabajo a una cola de ejecución es por medio del comando `sbatch`, para ésto tenemos que gerenar un script:\n",
        "\n",
        "`vasp.job:`\n",
        "```shell\n",
        "#!/bin/bash\n",
        "srun vasp_ncl\n",
        "```\n",
        "una vez creado el script se somete el script con el comando `sbatch`de la sigueinte forma:\n",
        "\n",
        "```shell\n",
        "$ sbatch --ntasks=16 --nodes=1 --partition=cpu vasp.job\n",
        "```\n",
        "```\n",
        "Submitted batch job 176\n",
        "```\n",
        "ésto nos regresa un número `176` el cual es el `ID` del trabajo.\n",
        "\n",
        "podemos agregar las opciones de la linea de comando de `sbatch` al mismo script, junto con otras opciones, de la siguiente forma:\n",
        "\n",
        "`vasp.job:`\n",
        "```shell\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=VASP_test        # Nombre para identificar el trabajo\n",
        "#SBATCH --nodes=1                   # utilizar cólo un nodo\n",
        "#SBATCH --ntasks=16                 # ejecitar 16 tareas\n",
        "#SBATCH --cpus-per-task=1           # CPUs por tarea\n",
        "#SBATCH --ntasks-per-node=16        # tareas por nodo\n",
        "#SBATCH --mem-per-cpu=2048          # cantidad máxima de memoria alojada por cpu en MB\n",
        "#SBATCH --output vasp_test-%j.o     # el nombre del archivo a donde escribir las salidas de la ejecución\n",
        "#SBATCH --error  vasp_test-%j.e     # nombre del archivo para escribir los errores de ejecución\n",
        "# %j se refiere al ID asignado a la hora de someter el script\n",
        "#SBATCH --partition=cpu             # partición en la cual ejecutar el script\n",
        "\n",
        "\n",
        "module load vasp/6.4.1-intel        # cargar el módulo adecuado\n",
        "\n",
        "srun vasp_ncl                      # ejecutar vast_ncl en los recursos solicitados\n",
        "```\n",
        "para someter éste último script sólo tenemos que mandar como parametro el nombre del script (`vasp.job`) a `sbatch`:\n",
        "```shell\n",
        "$ sbatch vasp.job\n",
        "```\n",
        "```shell\n",
        "Submitted batch job 177\n",
        "```\n",
        "\n",
        "<a name=\"squeue\"></a>\n",
        "## 3.4 squeue\n",
        "\n",
        "Para ver el éstado de ejecución de los trabajos en la cola podemos utilizar la instrucción `squeue`.\n",
        "\n",
        "```shell\n",
        "$ squeue\n",
        "```\n",
        "```shell\n",
        "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "             164         cpu  model_4   jperez  R 16-10:19:13     1 compute-0-3\n",
        "             174         cpu    model   jperez  R 4-07:57:35      1 compute-0-2\n",
        "             173         cpu     VASP vgalingo  R 2-12:45:11      1 compute-0-1\n",
        "             175         cpu     Test  wmedina  R   22:59:14      1 compute-0-2\n",
        "             171         cpu      aex  wmedina  R 1-20:57:02      1 compute-0-3\n",
        "             172         cpu   10-35s   clfdez  R    7:05:41      1 compute-0-1\n",
        "             177         cpu VASP_tes    suser  R      16:32      1 compute-0-1\n",
        "             178         cpu model_dm  emarmol  R    6:06:50      1 compute-0-3\n",
        "             182         cpu model_fm vgalindo  R    5:47:16      1 compute-0-4\n",
        "             184         cpu      igc  ncampos  R      56:32      1 compute-0-1\n",
        "             186         mem Na2SO2_d    jvzmg  R    7:05:41      1 compute-0-1\n",
        "```\n",
        "podemor verificar que nuestro trabajo esta corriendo (R), podemos ver sólo nuestros trabajos con la opción `--user $USER`\n",
        "\n",
        "```shell\n",
        "$ squeue --user $USER\n",
        "```\n",
        "```shell\n",
        "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "             177         cpu VASP_tes    suser  R      18:22      1 compute-0-1\n",
        "```\n",
        "\n",
        "<a name=\"scancel\"></a>\n",
        "## 3.5 scancel\n",
        "se puede cancelar el trabajo con el comando `scancel`:\n",
        "```shell\n",
        "$ scancel 177\n",
        "```\n",
        "``` shell\n",
        "$ squeue -u $USER\n",
        "```\n",
        "```shell\n",
        "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "             177         cpu VASP_tes    suser  CA      18:22      1 compute-0-1\n",
        "```\n",
        "podemos ver que el trabajo cambió de estado a cancelado (CA). También podemos utilizar el nombre el trabajo `VASP_test` con la opción `--name`,\n",
        "```shell\n",
        "$ scancel --name VASP_test\n",
        "```\n",
        "<a name=\"salloc\"></a>\n",
        "## 3.6 salloc\n",
        "\n",
        "También podemos ejecutar nuestros programas de una forma un poco más interactiva utilizando el comando `salloc`, el cual nos aloja los recursos solicitados y nos da un shell ineractivo\n",
        "```shell\n",
        "$ salloc -p cpu -N 1 -n 16 -J VASP_test -t 60\n",
        "```\n",
        "```shell\n",
        "salloc: Granted job allocation 178\n",
        "$\n",
        "```\n",
        "una vez terminada nuestra ejecución o pruebas nos podemos salir como en cualquier shell,\n",
        "```shell\n",
        "$ exit\n",
        "```\n",
        "```shell\n",
        "salloc: Relinquishing job allocation 178\n",
        "```\n"
      ],
      "metadata": {
        "id": "FrAD26NQ6xBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 4. Referencias:\n",
        "\n",
        "The SLURM website: https://slurm.schedmd.com/\n",
        "\n",
        "The SLURM documentation: https://slurm.schedmd.com/documentation.html\n",
        "\n",
        "The SLURM user community: https://groups.google.com/g/slurm-users"
      ],
      "metadata": {
        "id": "QX0OqurV6UzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aldo Rodríguez Guerrero, 2024"
      ],
      "metadata": {
        "id": "rF1IJkYp6kmY"
      }
    }
  ]
}
